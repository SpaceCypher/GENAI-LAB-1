{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9qFM9TfLF_2"
   },
   "source": [
    "# Unit 1 – Model Benchmark Challenge\n",
    "\n",
    "## Objective\n",
    "The goal of this notebook is to compare the behavior of three transformer architectures:\n",
    "- BERT (Encoder-only)\n",
    "- RoBERTa (Encoder-only)\n",
    "- BART (Encoder–Decoder)\n",
    "\n",
    "Each model is forced to perform tasks it may not be architecturally suited for, in order to observe failures and explain why architecture matters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciK--2xwLLPy"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers torch sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ys31HndFLNFc"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "g3FV_lO6LOri"
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"BERT\": \"bert-base-uncased\",\n",
    "    \"RoBERTa\": \"roberta-base\",\n",
    "    \"BART\": \"facebook/bart-base\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ckk5GSWLTXC"
   },
   "source": [
    "## Experiment 1: Text Generation\n",
    "\n",
    "**Prompt:**  \n",
    "\"The future of Artificial Intelligence is\"\n",
    "\n",
    "**Expected Behavior:**  \n",
    "Encoder-only models (BERT, RoBERTa) should fail or behave poorly because they are not trained for autoregressive text generation.  \n",
    "BART, which has a decoder, should generate more coherent output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "9343a3b169724094a77f4736bd8af602",
      "b47113c678b04a18984ea252e7d42f6d",
      "fd030804cf0d45518ee3f2a7aa7bb235",
      "dfd90025cb374ad486772cb0289149dd",
      "b45a6a232ab44036acb81deb78ebf4ec",
      "9b79ecace2ea49ada951bc603f3cdc6b",
      "0bc6092c29e64f3f91e4026d0508c29f",
      "cd8fd7c7342f442eacb66db7bd19b4b1",
      "36199fe8284a461f92b6e16ddbacc3ec",
      "05419ebd562a472bbd03a2af2dabccec",
      "fdb39e55837d4249acefc8035b744743",
      "bd846352c62348ab9d7f60d33942c0db",
      "c533c2db7be4481397e93a611c7a081f",
      "5139cc047ec749b58170f3f1cba20670",
      "1ddf38c3d7b249c69f86288a45491459",
      "e6ea69cb95d14515bb7a854c44feb7c0",
      "cbe35bc693bf400991d4d3c47a2c4a1d",
      "30688ae3ea4940d489743a6b17f08706",
      "e038ce35fe7a403d82f68e17ffd91786",
      "66540333aed241f58987ea8ed02c8f62",
      "ae9ee564454f4bcebc91cb304850e3b0",
      "42bde422bc74404abd61dc0d8b114d88",
      "b7922dc144b94efe837021423b4460d9",
      "bfcc948c721d4f4b930779d37fbb28f3",
      "16ac8567dfcf4efb99d6f09c9e9aa46a",
      "47084c0222434e3abc02ec13c8b8207e",
      "36f3ecdf5d014e23a421b2ed18f2a290",
      "bdc5f79f964c4fcebc807a740a747dcc",
      "9c9d5ba2aaed42b388d1dadf95dd0bdf",
      "b4be97016b4b473ca66b91d3716ffec8",
      "154e16933ef04aec9c05161b46b376ec",
      "0a0274c6f9334c19ba5ada99bb3fcaa7",
      "a41c6f4365f847cfbabe4c5e10064512",
      "53bbbf36ccd54044ad498b425d735d41",
      "4fdf029a34a04012a5c24e0136140203",
      "954af9cc346444a992a792a236acde5d",
      "8dd535caef304315be8ab7c3b68744ae",
      "8b183ef4985845ba9284e1a5ce20d283",
      "76783ebd43804702988940e5560431dc",
      "218471d688c2409e8f91a88896826282",
      "59b5b7e906c846dfb00f761318ebcba1",
      "2a0aac75470f4491ac9a8a7a7b8303ab",
      "e328ec9c6f024856b00709b87f808720",
      "cceeaff94603435993083463e19b4f00",
      "b75af2372b0441958248beb73afbe539",
      "51ccfd2920674cd1b38f7cfcde46bb1f",
      "ac448448ceda4b7eafe3f65e886088cb",
      "ea719eec0777491f9961607f19ab6ff7",
      "9d919ef25d0a4200b1a364763661dc92",
      "0586d5f15b604eb291e42539adc4968e",
      "44c1d57a361045f5bf201162e0ddc181",
      "4b46893d8b7b407ca7eb7d663392ef0b",
      "fed291682f4e455eaad0193c41c55dca",
      "bdef5015e81b4dc2ba638cfb355bc260",
      "a27c2733d2504c258447a11ced710bc4",
      "fe765ff68ce74bc8a5c671c7ac59fccd",
      "a26256d033f84d6ea9aee78fdee8d43e",
      "ef7e5a56e66646e4b0525068ca1cbc95",
      "ba4dc0f15b174fd0895c6fdf76e51b70",
      "be9c85c848584a508aa7c961cc964930",
      "8ca053eefff64aebbe86558ce7805b4f",
      "a0e489ec54ff4bb799b92d724cf2f4f4",
      "2a31ef9e21e64b9eb6539b655b683597",
      "36b10f57073e4ad0b07406f3b66193ec",
      "25bbcc771e7642e284d3895b572761be",
      "53fe0132af2c409bb7e333eeeefe3fff",
      "1a98e57082c346c2a4ab27fa17028746",
      "ef52d23495d9499881588e0963c3ad18",
      "3a0395d8f4c4427481bab1024f6a3473",
      "fb0316874a824d019a0637212ac2076b",
      "68a2cf4e41ea43d7a35aa1fc34a4e02c",
      "51ac008b9c904806b50655a7e9150616",
      "5143684c5d594984b62e18d824d5d66c",
      "01cb0f01d28f4efe818bb4bc8ec7e9c3",
      "1fbb2df31bc54034ae648916df3019e8",
      "55fd44eec88b4c7bad90089f725c0dd8",
      "54ca43b082084e429aa2f4ba9b859617",
      "b6d6531562144e158e12ffb7dd618899",
      "d1962909f1e244eb81f2a50785d72a17",
      "f1d7f509942f416b956b99f8afc355f2",
      "5b7003a1097a4be2bca244912034d468",
      "bc17270d39584727a8ed656cbef10cec",
      "d32323f64dbc4a278035cfd626eec522",
      "7f5260d16ab546c4856e9f6433c7a3e8",
      "d275b3da4ff4460aab0ac48ba2697c44",
      "479f13e5cea643cdac281bff54c4d09e",
      "fe52a243aebc4f2b896aa971bc1133db",
      "df358d7be9954ace9fd94007217df00b",
      "840f56f10c1c4868943ac1fc2465e9d2",
      "0b86c00f7e9745fe9e73eccc38641361",
      "149f14bf778f4d08bae44004b7b4e9bd",
      "e7181c80de44456e85a94229055299c6",
      "e5aa3c82102548809f47c0a28998cb82",
      "8297afb3a31444d6acfc5d942802193c",
      "7ec9b8a8cd604a9f99b6d6e894da5250",
      "e9491736619c4770a01f85fa4849c5ed",
      "e606d58d4b6a46638074fdcede5c4867",
      "e7eb46a2b757442089bc407453c44766",
      "14aead748ae24434b76fdc4bbc0b56e8",
      "ce7406b4fd7c4ba7b69e207cf405664d",
      "f1a257da49f4400fa584cf6597fd019a",
      "27b936ba3ab84d519b13ad28a4c86c7b",
      "6a47a285067f43dd84c29e16e785f8d3",
      "d8ec94d8ba81437d925c9d677aae027d",
      "05107dfb8d094d61b8096c55ff98fdae",
      "d1652f7579094ee6be0853102aff1d01",
      "710f1d3d02b5499790c45abdd5209a98",
      "64734f91f5b04cc2bc75fd2be56d0456",
      "b465525b87954a7d87236479becb253e",
      "c012d4880bff46aea15e7a1f4430f56e",
      "a4df1e8fd3fc42fc8e45d5e1b453866c",
      "bb137207a1194d7780fe0235c37e9c8d",
      "8770685f863441148b429f73dd0f8369",
      "73cebff4d35b48d8b892f523bcbab70f",
      "3635c747278f4326acb3d0ac1336424e",
      "69622a5c532245de9ec92a7e289d0c0d",
      "688cef8c4ce443df87aa455e802e5d6e",
      "9f1ff412d3ef42fe8bedd86d7124cc6f",
      "93d560c0ab3349aabbc747bba50c99e5",
      "64241d9b6378448babc61f797d7b09b7",
      "67d45c7c96494b56ad5a53c428d3791a",
      "f86e14bd4c114d1dab2ba6ecf20f5fa8",
      "14bf6837838e4a2a86570ca7aab540c9",
      "6ecde4e7194e4bcfa6c27b8f2f74b14b",
      "b0ba839c9e784b30a9a4c26aa989b78c",
      "0c5003ad7eb647888cb4d18fe56fe494",
      "0d03e61b9acb44828b9e2d7def7d6224",
      "4fd3e0b536ab403186f8c0a27438a932",
      "6d15baf4134444988878052e6ec4bde8",
      "b0f1b95290d04cc8b412d146025d7f31",
      "fefad59e237844848b25f62b963ad7f4",
      "035059c29ff34ff1aa29def39854e62d",
      "017fe00e578d4b23a1b4f6319865746c",
      "1108fea6a7d841b689ab66391320edc9",
      "cdd25de10e184e559338d4f47eb0f8d2",
      "46aab8528042492b8f2d00f7e1e11f45",
      "1d62ffe97519486c9e5330496ecb6b3d",
      "1681ecf56166492ab72db51fe39152d1",
      "be2e1d38f4b64dfbbd42761ecceea77c",
      "9848a0c3b16a4e76b2d40b551a8f1d6d",
      "000ec5fb4cf141bd9e40e3fcf4a10be3",
      "528e9cd6584642d18266b5426fcf7487",
      "63061708f4a54204b2aa84645a2e8fe0",
      "b4e3a811a46e460ca33c72ade36a6fc1",
      "5902c33f66c7412990f12388cab6dcb7",
      "f7068ec707a24639b100a76d7dc22a34",
      "86f5a42dbba64c4c9ff5ffbce0855bbf",
      "b558bae4466947fcb0915bd049ab7d84",
      "4cc9cd64a47340b19da413e2fb588ea4",
      "aef2b6ae393c4c0c9b12209a30e9964c",
      "5c2f644003a04f30a6db6b14edd607c1",
      "9603d23b5cb2468f823ada4b92d0d46c",
      "a4ffbe75ef9645a696983f3455440964",
      "5bfb4862bada4d0caa89027eedd897b6",
      "b2168d670d214cc9851592e100817263",
      "c70a535287d648c9a451ffa87fca9311",
      "5cd33d3d862f4aa088e906a0cbd280cf",
      "20ca071bb4094557983445a4eb5b4526",
      "d78555a5ab8d4c659e3b8a337c6f0c7a",
      "c1bc8fc6b2744c0eb065985616f6233b",
      "cc4c4271f85e499682dbe3ace48d5112",
      "9bd7d7b04efa41f18045a352ea92bc86",
      "95393ad62f23422ca4b94efd8c8f6a7b",
      "7201dd1ecb4b42369ed17195e80e3033",
      "2056009db7d54c668b1010be9445b775",
      "8ef3341751984840bbdecc8d9dcc2f12",
      "c5eed482472f474da5d188050619fbe9",
      "e546e0721a7a4a3d8f4db584d5554c22",
      "b8807abc2b2b47db8e83b00e5abd4030",
      "f89e8f6d863a438282aec7623b1ad04d",
      "9b6103a317b246d282208117ff7d2894",
      "0ea3157cc5f2403d9239eebbd0668bb0",
      "d54a040f777c43e89df4dc2f38971d99",
      "3516dbdefd554243af8d1d16ab69a703",
      "d2d68bc70d124f7496ca514cda99764f",
      "8e2d1034e423487e9f30d8c568e12fb2"
     ]
    },
    "id": "imfdnh3cLZ6E",
    "outputId": "0e62542c-e7cc-42a4-f56f-ba336945fa0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9343a3b169724094a77f4736bd8af602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd846352c62348ab9d7f60d33942c0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7922dc144b94efe837021423b4460d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bbbf36ccd54044ad498b425d735d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75af2372b0441958248beb73afbe539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n",
      "\n",
      "Model: RoBERTa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe765ff68ce74bc8a5c671c7ac59fccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a98e57082c346c2a4ab27fa17028746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d6531562144e158e12ffb7dd618899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840f56f10c1c4868943ac1fc2465e9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7406b4fd7c4ba7b69e207cf405664d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4df1e8fd3fc42fc8e45d5e1b453866c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is'}]\n",
      "\n",
      "Model: BART\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86e14bd4c114d1dab2ba6ecf20f5fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017fe00e578d4b23a1b4f6319865746c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e3a811a46e460ca33c72ade36a6fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2168d670d214cc9851592e100817263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef3341751984840bbdecc8d9dcc2f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is squadscoinscoins contemplating Violence diary diary diary robber uninstall Clarence Shah Shah Disney Disney Disney diary Shah monks diary Disney Disney idLinks monksPercentrx robbercc births DNA publishingOverrideframework births Utilities DNA DNA ContributionsPercent Rudd diary Funny USE diary Printed compression compression alterations diary charism id diary Girls diary diaryFeartoggleker diary Funnytoggle idQaeda Republican monks diary subsections DNA DNA Republican Chevy DNA cannabisoenix Republicanoenixoenix booking suggest compression pits {\\\\ Republican DNA wants wants compression Bre Utilities Funny wants DNA Proc Bre HaitiADRA compressionitals matured matured {\\\\ Peng wants alterations prestige wants compression wants agenda diary prestigeker diary wants DNA Kimmel wants wantsoming diary Bre dissidents wants matured diary diarychanges Republican riff diary diaryravis alarm Republican Republican Republican diary Cum Cumitals Republican diary dissatisfactionitals Barcl Republican dissidents cannabis Republican diary wants wants Peng charism Bre scal matured Haiti Republican diary diary Republicanulty diary compression Republican Finnish Republican compression Republicanitals Republican Paul DNA Republican Republican cannabistoggle reality Phillipsulty sarcastic Republican Republicanitalsitals Republicanitals wants Republican diaryulty prestigetoggle diary Republican Republican matured Republican Peng Republican Republican Goldman Goldmananton births Republican Republicanulty Goldman accept Goldman compression Goldman maturedanton Goldman Goldman Goldman Apex Serve matured Republican Paul diary Republican Cum RepublicanPod Goldman Republican Republican compressionulty diary Republican matured matured Apex Barclulty Republican Republican Cumulty Goldman matured Republican Republicaniving Republicantoggle Republicanulty'}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    try:\n",
    "        generator = pipeline(\"text-generation\", model=model)\n",
    "        output = generator(prompt, max_length=30)\n",
    "        print(output)\n",
    "    except Exception as e:\n",
    "        print(\"FAILED:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1oZvDo-LfsF"
   },
   "source": [
    "## Experiment 2: Masked Language Modeling (Fill-Mask)\n",
    "\n",
    "**Sentence:**  \n",
    "\"The goal of Generative AI is to [MASK] new content.\"\n",
    "\n",
    "**Expected Behavior:**  \n",
    "BERT and RoBERTa should perform well because they are trained using Masked Language Modeling (MLM).  \n",
    "BART may perform inconsistently because MLM is not its primary training objective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3o1KTPkwLimx",
    "outputId": "e421b0bd-58d1-4d6e-d441-6cf59e594792"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.5396924614906311, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575772523880005, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}, {'score': 0.054054826498031616, 'token': 3965, 'token_str': 'produce', 'sequence': 'the goal of generative ai is to produce new content.'}]\n",
      "\n",
      "Model: RoBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED: No mask_token (<mask>) found on the input\n",
      "\n",
      "Model: BART\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED: No mask_token (<mask>) found on the input\n"
     ]
    }
   ],
   "source": [
    "masked_sentence = \"The goal of Generative AI is to [MASK] new content.\"\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    try:\n",
    "        fill_mask = pipeline(\"fill-mask\", model=model)\n",
    "        output = fill_mask(masked_sentence)\n",
    "        print(output[:3])  # top 3 predictions\n",
    "    except Exception as e:\n",
    "        print(\"FAILED:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exS0lz21Lltv"
   },
   "source": [
    "## Experiment 3: Question Answering\n",
    "\n",
    "**Question:**  \n",
    "\"What are the risks?\"\n",
    "\n",
    "**Context:**  \n",
    "\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "\n",
    "**Expected Behavior:**  \n",
    "Since these are base models and not fine-tuned on QA datasets like SQuAD, results may be incomplete, incorrect, or random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQ85zKsYLneQ",
    "outputId": "0d7f2efc-fe59-43aa-c392-0d62ee3d040d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.007372487103566527, 'start': 46, 'end': 60, 'answer': 'hallucinations'}\n",
      "\n",
      "Model: RoBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.012141286628320813, 'start': 72, 'end': 82, 'answer': 'deepfakes.'}\n",
      "\n",
      "Model: BART\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.04201485961675644, 'start': 14, 'end': 61, 'answer': 'poses significant risks such as hallucinations,'}\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the risks?\"\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    try:\n",
    "        qa = pipeline(\"question-answering\", model=model)\n",
    "        output = qa(question=question, context=context)\n",
    "        print(output)\n",
    "    except Exception as e:\n",
    "        print(\"FAILED:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzEd32XsMTI3"
   },
   "source": [
    "## Observation Table\n",
    "\n",
    "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Text Generation** | BERT | Failure | The model failed to generate fluent text and produced incoherent output or warnings about missing decoder settings. | BERT is an encoder-only model trained for understanding text, not for autoregressive next-token generation. |\n",
    "| | RoBERTa | Failure | Similar to BERT, the model could not generate meaningful text and largely echoed the prompt. | RoBERTa is also an encoder-only architecture and lacks a decoder for sequential text generation. |\n",
    "| | BART | Success | The model generated a continuation of the prompt, though the text was noisy and repetitive. | BART uses an encoder–decoder architecture with a decoder capable of sequence generation. |\n",
    "| **Fill-Mask** | BERT | Success | The model correctly predicted words such as “create” and “generate” for the masked token. | BERT is trained using Masked Language Modeling (MLM), making it well-suited for this task. |\n",
    "| | RoBERTa | Failure | The model failed due to the absence of the required `<mask>` token in the input sentence. | RoBERTa uses a different mask token (`<mask>`) than BERT, causing a tokenizer-level mismatch rather than a model limitation. |\n",
    "| | BART | Failure | The model failed to perform the fill-mask task and raised errors related to mask handling. | BART is not designed for traditional word-level MLM tasks; it is trained on sequence corruption and reconstruction instead. |\n",
    "| **Question Answering** | BERT | Failure | The model failed to produce a meaningful answer and issued warnings about uninitialized QA layers. | Question answering requires fine-tuning on QA datasets (e.g., SQuAD), which base BERT lacks. |\n",
    "| | RoBERTa | Failure | The extracted answer was incomplete and unreliable. | RoBERTa base models are not fine-tuned for extractive question answering tasks. |\n",
    "| | BART | Partial | The model extracted relevant spans from the context but with low confidence and inconsistency. | Although BART can generate text, it still requires QA-specific fine-tuning for reliable question answering. |\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
